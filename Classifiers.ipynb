{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politeness Classifiers\n",
    "\n",
    "### Factors outlined as contributing to politeness ratings for the data examples:\n",
    "\n",
    "Direct Questions\n",
    "\n",
    "Factuality\n",
    "\n",
    "Please\n",
    "\n",
    "Hedging\n",
    "\n",
    "Counterfactual\n",
    "\n",
    "Deference\n",
    "\n",
    "#### TODO: Ablate these features using those extracted from Politeness_Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "labels = ['ID', 'Message', 'NS', 'NNS']\n",
    "filenames = [\"BinaryLabeling.csv\", \"StrongNeutralLabeling.csv\",\n",
    "             \"WeakNeutralLabeling.csv\", \"IntermediateLabeling.csv\",\n",
    "            \"PartitionsLabeling.csv\"]\n",
    "fileobjs = [open(\"LabeledData/\" + i, \"r\") for i in filenames]\n",
    "readers = [csv.reader(i) for i in fileobjs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier: Unigrams\n",
    "\n",
    "This will be a baseline classifier for our labeling schemes, using a simple Bag of Words approach to determine labels based purely off of words present in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.66\n",
      "non-native speaker:\n",
      "0.7066666666666667\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.4866666666666667\n",
      "non-native speaker:\n",
      "0.4866666666666667\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.7\n",
      "non-native speaker:\n",
      "0.5933333333333334\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.5\n",
      "non-native speaker:\n",
      "0.4866666666666667\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.2866666666666667\n",
      "non-native speaker:\n",
      "0.3466666666666667\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "from collections import Counter\n",
    "\n",
    "# Create featureset from all individual words in training\n",
    "next(readers[0], None)\n",
    "num_train = 900 # Training comes from first 900 of 1000 samples\n",
    "all_words = set()\n",
    "for row in readers[0]:\n",
    "    if num_train <= 0:\n",
    "        break;\n",
    "    line = word_tokenize(row[1])\n",
    "    for word in line:\n",
    "        all_words.add(word)\n",
    "    num_train -= 1\n",
    "\n",
    "# Using seek(0) resets reader\n",
    "fileobjs[0].seek(0)\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    d = dict.fromkeys(all_words, 0)\n",
    "    c = Counter(word_tokenize(sentence))\n",
    "    for i in c:\n",
    "        d[i] = c[i]\n",
    "    return d\n",
    "\n",
    "NB_classifiers_NS = []\n",
    "NB_classifiers_NNS = []\n",
    "NB_tests_NS = []\n",
    "NB_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    all_data = list(i)\n",
    "    train_NS = [(bag_of_words(row[1]), row[2]) for row in all_data[:850]]\n",
    "    train_NNS = [(bag_of_words(row[1]), row[3]) for row in all_data[:850]]\n",
    "    NB_tests_NS.append([(bag_of_words(row[1]), row[2]) for row in all_data[850:]])\n",
    "    NB_tests_NNS.append([(bag_of_words(row[1]), row[3]) for row in all_data[850:]])\n",
    "\n",
    "    NB_classifiers_NS.append(NaiveBayesClassifier.train(train_NS))\n",
    "    NB_classifiers_NNS.append(NaiveBayesClassifier.train(train_NNS))\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(accuracy(NB_classifiers_NS[i], NB_tests_NS[i]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(accuracy(NB_classifiers_NNS[i], NB_tests_NNS[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier: Base Prediction Model\n",
    "\n",
    "Per the slides, we want to build a logistic regression model using three main measures:\n",
    "perspective API scores (~ toxicity), readability measures, and length of sample\n",
    "\n",
    "### Issue with the perspective API scores:\n",
    "\n",
    "The API has a limited amount of queries per minute for our feature collection. To combat this, a loop has been put in that waits when such an error occurs. However, this means the featureset of the data takes a very large amount of time because of all the waiting around we have to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import textstat\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Variables for perspective API call\n",
    "# headers and parameters for perspective api call\n",
    "api_key = 'AIzaSyBaMPpybrBfyWF54hvkFK1QuEBPPKmQh8M'\n",
    "url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' +    \n",
    "    '?key=' + api_key)\n",
    "\n",
    "# Since readability returns string of form \"xth to (x+1)th grade\",\n",
    "# we should only grab the first one.\n",
    "def find_first_num(s):\n",
    "    i = re.search('[0-9]+', s).group()\n",
    "    return int(i)\n",
    "\n",
    "def features(sentence):\n",
    "    d = {}\n",
    "    d['readability'] = find_first_num(textstat.text_standard(sentence))\n",
    "    d['length'] = len(word_tokenize(sentence))\n",
    "    \n",
    "    # preprocessing text to make readable for perspective api scores:\n",
    "    text = ''\n",
    "    for a in sentence:\n",
    "        if a==' ' or (a<='Z' and a>='A') or (a<='z' and a>='a') or (a<='9' and a>='0') or a=='?' or a=='.':\n",
    "            text +=a\n",
    "\n",
    "    # perspective api scores call:\n",
    "    data = '{comment: {text:\"'+text+'\"}, languages: [\"en\"], requestedAttributes: {TOXICITY:{}} }'\n",
    "    response = requests.post(url=url, data=data)\n",
    "    j = json.loads(response.content)\n",
    "    # attempting to deal with API issues\n",
    "    while 'error' in j:\n",
    "        time.sleep(5)\n",
    "        response = requests.post(url=url, data=data)\n",
    "        j = json.loads(response.content)\n",
    "    try:\n",
    "        d['toxicity'] = float(j['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "    except:\n",
    "        d['toxicity'] = 0.0\n",
    "    assert(len(d.values()) == 3)\n",
    "    return d\n",
    "\n",
    "fileobjs[0].seek(0)\n",
    "# Creating feature dict for each sample in dataset\n",
    "next(readers[0], None)\n",
    "all_data = list(readers[0])\n",
    "feature_data = {}\n",
    "for row in all_data:\n",
    "    feature_data[row[0]] = features(row[1])\n",
    "fileobjs[0].seek(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_process(num_features):\n",
    "    # Creating matrix of (samples, features) for sklearn models\n",
    "    feature_matrix = []\n",
    "    for i in range(1,1001):\n",
    "        feature_matrix.append(list(feature_data[str(i)].values()))\n",
    "    feature_matrix = numpy.array([numpy.array(x) for x in feature_matrix])\n",
    "    for i in feature_matrix:\n",
    "        if len(i) != num_features:\n",
    "            print(i) # debugging in case perspective api fails\n",
    "    return numpy.stack(feature_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.64\n",
      "non-native speaker:\n",
      "0.6\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.43\n",
      "non-native speaker:\n",
      "0.39\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.72\n",
      "non-native speaker:\n",
      "0.72\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.61\n",
      "non-native speaker:\n",
      "0.57\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.19\n",
      "non-native speaker:\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "feature_matrix = data_process(3)\n",
    "\n",
    "L_classifiers_NS = []\n",
    "L_classifiers_NNS = []\n",
    "L_tests_NS = []\n",
    "L_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    # Easier to use DataFrame obj to work with skl models\n",
    "    data_NS=pd.DataFrame({\n",
    "        'readability':feature_matrix[:,0],\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['readability', 'length', 'toxicity']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 90% training, 10% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    L_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    L_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    L_classifiers_NS.append(clfNS)\n",
    "    L_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(L_classifiers_NS[i].score(L_tests_NS[i][0], L_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(L_classifiers_NNS[i].score(L_tests_NNS[i][0], L_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Observations\n",
    "\n",
    "A naive hypothesis would assume higher accuracy for less expressive labeling schemes, but this does not always seem to be the case.\n",
    "\n",
    "In terms of accuracy, we have our Weak Neutral with the highest and Strong Neutral at the lowest. What is interesting is that the Binary and Intermediate Labeling schemes have very similar accuracies, despite being farthest apart in terms of expressiveness.\n",
    "\n",
    "### A big deciding factor of which labeling schema has the highest accuracy, appears to be how 'neutral' is expressed.\n",
    "\n",
    "EDIT: after adding partitions-based labeling, it seems to have the lowest accuracy, decreasing as we move from the Naive Bayes Classifier to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Additional Features\n",
    "\n",
    "### Adding in politeness score (from work by Prof. Danescu-Niculescu-Mizil)\n",
    "\n",
    "We are importing code from another repo focused on measuring politeness on emails.\n",
    "\n",
    "Possible issues with this approach:\n",
    "\n",
    "- Does not give a singular value measuring both politeness and impoliteness. Splitting up the scoring of a text into a separate politeness and impoliteness score might skew model results.\n",
    "\n",
    "- Words labeled as \"negative\" or \"profane\" can often be too generally applied, as they might be contained in the text but not in an offensive context. For example, the word \"black\" can be offensive in a racial context, but is often used just as a color for inanimate objects.\n",
    "\n",
    "To remedy this, the features that go into this calculation have been extracted and added into the feedback:\n",
    "\n",
    "- Please start: Sentence beginning with 'Please'.\n",
    "- 1st person pl.: Sentence using the first-person plural after the start ('us' or 'we').\n",
    "- Deference: Showing praise or compliment.\n",
    "- SUBJUNCTIVE: Presence of subjunctive tense.\n",
    "- 1st person start: Sentence starts with the first person pronoun.\n",
    "- Factuality: Discussing of events in a factual manner (e.g. 'in fact').\n",
    "- Hedges: Displaying hesitation or uncertainty (e.g. 'maybe' or 'suggest').\n",
    "- HASNEGATIVE: Uses a negative adverb or adjective.\n",
    "- Direct start: Directly delivers main topic or request (e.g. \"So, can you do this for me?\").\n",
    "- 1st person: Using the first person after the start.\n",
    "- Direct question: Addresses audience directly to ask something.\n",
    "- Apologizing: Saying sorry or offering submissive sincerity.\n",
    "- Indirect (greeting): Contains formal or informal greetings (e.g. 'Hey').\n",
    "- 2nd person start: Sentence starts with a second person pronoun.\n",
    "- HASPOSITIVE: Contains a very positive word.\n",
    "- INDICATIVE: Presents a statement using indicative modals like 'can' or 'will.\n",
    "- Please: Using the word 'please' after the start.\n",
    "- Gratitude: Expressing appreciation.\n",
    "- HASHEDGE: Contains hedging (very similar to other feature).\n",
    "- 2nd person: Contains second person pronouns after the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Please start', '1st person pl.', 'Deference', 'SUBJUNCTIVE', '1st person start', 'Factuality', 'Hedges', 'HASNEGATIVE', 'Direct start', '1st person', 'Direct question', 'Apologizing', 'Indirect (greeting)', '2nd person start', 'HASPOSITIVE', 'INDICATIVE', 'Please', 'Gratitude', 'HASHEDGE', '2nd person'}\n"
     ]
    }
   ],
   "source": [
    "from Politeness_Feedback.utils import *\n",
    "from Politeness_Feedback.politeness.api_util import get_scores_strategies_token_indices\n",
    "\n",
    "fileobjs[0].seek(0)\n",
    "# Adding impolite and polite scores into model\n",
    "next(readers[0], None)\n",
    "all_data = list(readers[0])\n",
    "p_feat_set = set()\n",
    "for row in all_data:\n",
    "    s = get_scores_strategies_token_indices(row[1])['strategies']\n",
    "    for i in s:\n",
    "        p_feat_set.add(i)\n",
    "print(p_feat_set)\n",
    "\n",
    "fileobjs[0].seek(0)\n",
    "next(readers[0], None)\n",
    "all_data = list(readers[0])\n",
    "for row in all_data:\n",
    "    r = score_text(row[1])\n",
    "    # Adding impolite and polite scores into model\n",
    "    feature_data[row[0]]['score_impolite'] = r[1]\n",
    "    feature_data[row[0]]['score_polite'] = r[2]\n",
    "    # Adding extracted features into model\n",
    "    s = get_scores_strategies_token_indices(row[1])['strategies']\n",
    "    for i in p_feat_set:\n",
    "        feature_data[row[0]][i] = 1 if i in s else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.78\n",
      "non-native speaker:\n",
      "0.68\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.54\n",
      "non-native speaker:\n",
      "0.48\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.75\n",
      "non-native speaker:\n",
      "0.68\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.56\n",
      "non-native speaker:\n",
      "0.54\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.31\n",
      "non-native speaker:\n",
      "0.28\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "feature_matrix = data_process(25)\n",
    "\n",
    "P_classifiers_NS = []\n",
    "P_classifiers_NNS = []\n",
    "P_tests_NS = []\n",
    "P_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    # Adding huge amount of features to DataFrame\n",
    "    data_NS=pd.DataFrame({\n",
    "        'readability':feature_matrix[:,0],\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'score_impolite':feature_matrix[:,3],\n",
    "        'score_polite':feature_matrix[:,4],\n",
    "        'Please start':feature_matrix[:,5],\n",
    "        '1st person pl.':feature_matrix[:,6],\n",
    "        'Deference':feature_matrix[:,7],\n",
    "        'SUBJUNCTIVE':feature_matrix[:,8],\n",
    "        '1st person start':feature_matrix[:,9],\n",
    "        'Factuality':feature_matrix[:,10],\n",
    "        'Hedges':feature_matrix[:,11],\n",
    "        'HASNEGATIVE':feature_matrix[:,12],\n",
    "        'Direct start':feature_matrix[:,13],\n",
    "        '1st person':feature_matrix[:,14],\n",
    "        'Direct question':feature_matrix[:,15],\n",
    "        'Apologizing':feature_matrix[:,16],\n",
    "        'Indirect (greeting)':feature_matrix[:,17],\n",
    "        '2nd person start':feature_matrix[:,18],\n",
    "        'HASPOSITIVE':feature_matrix[:,19],\n",
    "        'INDICATIVE':feature_matrix[:,20],\n",
    "        'Please':feature_matrix[:,21],\n",
    "        'Gratitude':feature_matrix[:,22],\n",
    "        'HASHEDGE':feature_matrix[:,23],\n",
    "        '2nd person':feature_matrix[:,24],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['readability', 'length', 'toxicity', 'score_impolite', 'score_polite',\n",
    "               'Please start', '1st person pl.', 'Deference', 'SUBJUNCTIVE', '1st person start',\n",
    "               'Factuality', 'Hedges', 'HASNEGATIVE', 'Direct start', '1st person', 'Direct question',\n",
    "               'Apologizing', 'Indirect (greeting)', '2nd person start', 'HASPOSITIVE', 'INDICATIVE',\n",
    "               'Please', 'Gratitude', 'HASHEDGE', '2nd person']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    P_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    P_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    P_classifiers_NS.append(clfNS)\n",
    "    P_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(P_classifiers_NS[i].score(P_tests_NS[i][0], P_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(P_classifiers_NNS[i].score(P_tests_NNS[i][0], P_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "\n",
    "Given our current 4 features, we will be experimenting with taking them away one-at-a-time and retraining our models to see which ones are actually useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Reading Level\n",
    "\n",
    "Because the subjects were adults with high levels of English comprehension (even non-native speakers), we hypothesize that removing this feature will not remove model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.66\n",
      "non-native speaker:\n",
      "0.67\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.54\n",
      "non-native speaker:\n",
      "0.58\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.76\n",
      "non-native speaker:\n",
      "0.75\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.56\n",
      "non-native speaker:\n",
      "0.56\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.3\n",
      "non-native speaker:\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "no_read_classifiers_NS = []\n",
    "no_read_classifiers_NNS = []\n",
    "no_read_tests_NS = []\n",
    "no_read_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'score_impolite':feature_matrix[:,3],\n",
    "        'score_polite':feature_matrix[:,4],\n",
    "        'Please start':feature_matrix[:,5],\n",
    "        '1st person pl.':feature_matrix[:,6],\n",
    "        'Deference':feature_matrix[:,7],\n",
    "        'SUBJUNCTIVE':feature_matrix[:,8],\n",
    "        '1st person start':feature_matrix[:,9],\n",
    "        'Factuality':feature_matrix[:,10],\n",
    "        'Hedges':feature_matrix[:,11],\n",
    "        'HASNEGATIVE':feature_matrix[:,12],\n",
    "        'Direct start':feature_matrix[:,13],\n",
    "        '1st person':feature_matrix[:,14],\n",
    "        'Direct question':feature_matrix[:,15],\n",
    "        'Apologizing':feature_matrix[:,16],\n",
    "        'Indirect (greeting)':feature_matrix[:,17],\n",
    "        '2nd person start':feature_matrix[:,18],\n",
    "        'HASPOSITIVE':feature_matrix[:,19],\n",
    "        'INDICATIVE':feature_matrix[:,20],\n",
    "        'Please':feature_matrix[:,21],\n",
    "        'Gratitude':feature_matrix[:,22],\n",
    "        'HASHEDGE':feature_matrix[:,23],\n",
    "        '2nd person':feature_matrix[:,24],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['length', 'toxicity', 'score_impolite', 'score_polite', 'Please start',\n",
    "               '1st person pl.', 'Deference', 'SUBJUNCTIVE', '1st person start', 'Factuality',\n",
    "               'Hedges', 'HASNEGATIVE', 'Direct start', '1st person', 'Direct question',\n",
    "               'Apologizing', 'Indirect (greeting)', '2nd person start', 'HASPOSITIVE', 'INDICATIVE',\n",
    "               'Please', 'Gratitude', 'HASHEDGE', '2nd person']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    no_read_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    no_read_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    no_read_classifiers_NS.append(clfNS)\n",
    "    no_read_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(no_read_classifiers_NS[i].score(no_read_tests_NS[i][0], no_read_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(no_read_classifiers_NNS[i].score(no_read_tests_NNS[i][0], no_read_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Document Length\n",
    "\n",
    "We assert the experiment controlling, on average, attention spans of participants (both native and non-native). Coupled with our previous assumption on high levels of English from all participants, we hypothesize document length has a negligible effect on politeness ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.73\n",
      "non-native speaker:\n",
      "0.63\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.51\n",
      "non-native speaker:\n",
      "0.51\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.69\n",
      "non-native speaker:\n",
      "0.7\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.55\n",
      "non-native speaker:\n",
      "0.6\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.35\n",
      "non-native speaker:\n",
      "0.38\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "no_len_classifiers_NS = []\n",
    "no_len_classifiers_NNS = []\n",
    "no_len_tests_NS = []\n",
    "no_len_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'score_impolite':feature_matrix[:,3],\n",
    "        'score_polite':feature_matrix[:,4],\n",
    "        'Please start':feature_matrix[:,5],\n",
    "        '1st person pl.':feature_matrix[:,6],\n",
    "        'Deference':feature_matrix[:,7],\n",
    "        'SUBJUNCTIVE':feature_matrix[:,8],\n",
    "        '1st person start':feature_matrix[:,9],\n",
    "        'Factuality':feature_matrix[:,10],\n",
    "        'Hedges':feature_matrix[:,11],\n",
    "        'HASNEGATIVE':feature_matrix[:,12],\n",
    "        'Direct start':feature_matrix[:,13],\n",
    "        '1st person':feature_matrix[:,14],\n",
    "        'Direct question':feature_matrix[:,15],\n",
    "        'Apologizing':feature_matrix[:,16],\n",
    "        'Indirect (greeting)':feature_matrix[:,17],\n",
    "        '2nd person start':feature_matrix[:,18],\n",
    "        'HASPOSITIVE':feature_matrix[:,19],\n",
    "        'INDICATIVE':feature_matrix[:,20],\n",
    "        'Please':feature_matrix[:,21],\n",
    "        'Gratitude':feature_matrix[:,22],\n",
    "        'HASHEDGE':feature_matrix[:,23],\n",
    "        '2nd person':feature_matrix[:,24],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['toxicity', 'score_impolite', 'score_polite', 'Please start',\n",
    "               '1st person pl.', 'Deference', 'SUBJUNCTIVE', '1st person start', 'Factuality',\n",
    "               'Hedges', 'HASNEGATIVE', 'Direct start', '1st person', 'Direct question',\n",
    "               'Apologizing', 'Indirect (greeting)', '2nd person start', 'HASPOSITIVE', 'INDICATIVE',\n",
    "               'Please', 'Gratitude', 'HASHEDGE', '2nd person']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    no_len_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    no_len_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    no_len_classifiers_NS.append(clfNS)\n",
    "    no_len_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(no_len_classifiers_NS[i].score(no_len_tests_NS[i][0], no_len_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(no_len_classifiers_NNS[i].score(no_len_tests_NNS[i][0], no_len_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness vs Toxicity\n",
    "\n",
    "Both API for these features should be measuring with some similarity. By comparing accuracy between models using only one or the other, how similar are the two measures? In addition, we are creating a fine-grained approach to our classifier by picking apart the extracted politeness features separately.\n",
    "\n",
    "### Toxicity Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.61\n",
      "non-native speaker:\n",
      "0.56\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.42\n",
      "non-native speaker:\n",
      "0.36\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.78\n",
      "non-native speaker:\n",
      "0.7\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.62\n",
      "non-native speaker:\n",
      "0.54\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.31\n",
      "non-native speaker:\n",
      "0.22\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "justT_classifiers_NS = []\n",
    "justT_classifiers_NNS = []\n",
    "justT_tests_NS = []\n",
    "justT_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['toxicity']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    justT_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    justT_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    justT_classifiers_NS.append(clfNS)\n",
    "    justT_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(justT_classifiers_NS[i].score(justT_tests_NS[i][0], justT_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(justT_classifiers_NNS[i].score(justT_tests_NNS[i][0], justT_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.66\n",
      "non-native speaker:\n",
      "0.62\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.54\n",
      "non-native speaker:\n",
      "0.61\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.73\n",
      "non-native speaker:\n",
      "0.66\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.59\n",
      "non-native speaker:\n",
      "0.61\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.31\n",
      "non-native speaker:\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "justP_classifiers_NS = []\n",
    "justP_classifiers_NNS = []\n",
    "justP_tests_NS = []\n",
    "justP_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'score_impolite':feature_matrix[:,3],\n",
    "        'score_polite':feature_matrix[:,4],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['score_impolite', 'score_polite']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    justP_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    justP_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    justP_classifiers_NS.append(clfNS)\n",
    "    justP_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(justP_classifiers_NS[i].score(justP_tests_NS[i][0], justP_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(justP_classifiers_NNS[i].score(justP_tests_NNS[i][0], justP_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Extracted Features\n",
    "\n",
    "From here on, we want to ablate on these extracted features by category:\n",
    "\n",
    "Directness:\n",
    "- Please start\n",
    "- 1st person start\n",
    "- Direct start\n",
    "- Direct question\n",
    "- 2nd person start\n",
    "\n",
    "Positive/Negative:\n",
    "- HASPOSITIVE\n",
    "- HASNEGATIVE\n",
    "- Deference\n",
    "- Gratitude\n",
    "- Apologizing\n",
    "\n",
    "(Un)certainty:\n",
    "- SUBJUNCTIVE\n",
    "- HASHEDGE\n",
    "- INDICATIVE\n",
    "- Factuality\n",
    "- Hedges\n",
    "\n",
    "Structural Ambiguity\n",
    "- 1st person pl.\n",
    "- 1st person\n",
    "- Indirect (greeting)\n",
    "- Please\n",
    "- 2nd person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.58\n",
      "non-native speaker:\n",
      "0.71\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.5\n",
      "non-native speaker:\n",
      "0.44\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.65\n",
      "non-native speaker:\n",
      "0.69\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.54\n",
      "non-native speaker:\n",
      "0.6\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.29\n",
      "non-native speaker:\n",
      "0.35\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "extract_classifiers_NS = []\n",
    "extract_classifiers_NNS = []\n",
    "extract_tests_NS = []\n",
    "extract_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        # Directness\n",
    "        'Please start':feature_matrix[:,5],\n",
    "        '1st person start':feature_matrix[:,9],\n",
    "        'Direct start':feature_matrix[:,13],\n",
    "        'Direct question':feature_matrix[:,15],\n",
    "        '2nd person start':feature_matrix[:,18],\n",
    "\n",
    "        # Pos/Neg\n",
    "        'Deference':feature_matrix[:,7],\n",
    "        'HASNEGATIVE':feature_matrix[:,12],\n",
    "        'HASPOSITIVE':feature_matrix[:,19],\n",
    "        'Gratitude':feature_matrix[:,22],\n",
    "        'Apologizing':feature_matrix[:,16],\n",
    "        \n",
    "        # (Un)certainty\n",
    "        'SUBJUNCTIVE':feature_matrix[:,8],\n",
    "        'Factuality':feature_matrix[:,10],\n",
    "        'Hedges':feature_matrix[:,11],\n",
    "        'HASHEDGE':feature_matrix[:,23],\n",
    "        'INDICATIVE':feature_matrix[:,20],\n",
    "        \n",
    "        # Structural ambig.\n",
    "        '1st person':feature_matrix[:,14],\n",
    "        '1st person pl.':feature_matrix[:,6],\n",
    "        'Indirect (greeting)':feature_matrix[:,17],\n",
    "        'Please':feature_matrix[:,21],\n",
    "        '2nd person':feature_matrix[:,24],\n",
    "\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['Please start', '1st person pl.', 'Deference', 'SUBJUNCTIVE', '1st person start', 'Factuality',\n",
    "               'Hedges', 'HASNEGATIVE', 'Direct start', '1st person', 'Direct question',\n",
    "               'Apologizing', 'Indirect (greeting)', '2nd person start', 'HASPOSITIVE', 'INDICATIVE',\n",
    "               'Please', 'Gratitude', 'HASHEDGE', '2nd person']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    extract_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    extract_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    extract_classifiers_NS.append(clfNS)\n",
    "    extract_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(extract_classifiers_NS[i].score(extract_tests_NS[i][0], extract_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(extract_classifiers_NNS[i].score(extract_tests_NNS[i][0], extract_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
