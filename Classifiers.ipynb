{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politeness Classifiers\n",
    "\n",
    "### Factors outlined as contributing to politeness ratings for the data examples:\n",
    "\n",
    "Direct Questions\n",
    "\n",
    "Factuality\n",
    "\n",
    "Please\n",
    "\n",
    "Hedging\n",
    "\n",
    "Counterfactual\n",
    "\n",
    "Deference\n",
    "\n",
    "#### TODO: Implement features into classifier\n",
    "\n",
    "#### TODO: Implement features from Prof. DNM politeness, labeling based on frequency in sample (frequency statistics as well), ablation study (NS vs NNS -trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "labels = ['ID', 'Message', 'NS', 'NNS']\n",
    "filenames = [\"BinaryLabeling.csv\", \"StrongNeutralLabeling.csv\",\n",
    "             \"WeakNeutralLabeling.csv\", \"IntermediateLabeling.csv\",\n",
    "            \"PartitionsLabeling.csv\"]\n",
    "fileobjs = [open(\"LabeledData/\" + i, \"r\") for i in filenames]\n",
    "readers = [csv.reader(i) for i in fileobjs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier: Unigrams\n",
    "\n",
    "This will be a baseline classifier for our labeling schemes, using a simple Bag of Words approach to determine labels based purely off of words present in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.6666666666666666\n",
      "non-native speaker:\n",
      "0.7133333333333334\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.48\n",
      "non-native speaker:\n",
      "0.48\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.7266666666666667\n",
      "non-native speaker:\n",
      "0.64\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.5266666666666666\n",
      "non-native speaker:\n",
      "0.5133333333333333\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.2866666666666667\n",
      "non-native speaker:\n",
      "0.35333333333333333\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "from collections import Counter\n",
    "\n",
    "# Create featureset from all individual words in training\n",
    "next(readers[0], None)\n",
    "num_train = 850 # Training comes from first 850 of 1000 samples\n",
    "all_words = set()\n",
    "for row in readers[0]:\n",
    "    if num_train <= 0:\n",
    "        break;\n",
    "    line = word_tokenize(row[1])\n",
    "    for word in line:\n",
    "        all_words.add(word)\n",
    "    num_train -= 1\n",
    "fileobjs[0].seek(0)\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    d = dict.fromkeys(all_words, 0)\n",
    "    c = Counter(word_tokenize(sentence))\n",
    "    for i in c:\n",
    "        d[i] = c[i]\n",
    "    return d\n",
    "\n",
    "NB_classifiers_NS = []\n",
    "NB_classifiers_NNS = []\n",
    "NB_tests_NS = []\n",
    "NB_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    all_data = list(i)\n",
    "    train_NS = [(bag_of_words(row[1]), row[2]) for row in all_data[:850]]\n",
    "    train_NNS = [(bag_of_words(row[1]), row[3]) for row in all_data[:850]]\n",
    "    NB_tests_NS.append([(bag_of_words(row[1]), row[2]) for row in all_data[850:]])\n",
    "    NB_tests_NNS.append([(bag_of_words(row[1]), row[3]) for row in all_data[850:]])\n",
    "\n",
    "    NB_classifiers_NS.append(NaiveBayesClassifier.train(train_NS))\n",
    "    NB_classifiers_NNS.append(NaiveBayesClassifier.train(train_NNS))\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(accuracy(NB_classifiers_NS[i], NB_tests_NS[i]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(accuracy(NB_classifiers_NNS[i], NB_tests_NNS[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier: Base Prediction Model\n",
    "\n",
    "Per the slides, we want to build a logistic regression model using three main measures:\n",
    "perspective API scores (~ toxicity), readability measures, and length of sample\n",
    "\n",
    "### Issue with the perspective API scores:\n",
    "\n",
    "The API has a limited amount of queries per minute for our feature collection. To combat this, a loop has been put in that waits when such an error occurs. However, this means the featureset of the data takes a very large amount of time because of all the waiting around we have to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import textstat\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Variables for perspective API call\n",
    "# headers and parameters for perspective api call\n",
    "api_key = 'AIzaSyBaMPpybrBfyWF54hvkFK1QuEBPPKmQh8M'\n",
    "url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' +    \n",
    "    '?key=' + api_key)\n",
    "\n",
    "# Since readability returns string of form \"xth to (x+1)th grade\",\n",
    "# we should only grab the first one.\n",
    "def find_first_num(s):\n",
    "    i = re.search('[0-9]+', s).group()\n",
    "    return int(i)\n",
    "\n",
    "def features(sentence):\n",
    "    d = {}\n",
    "    d['readability'] = find_first_num(textstat.text_standard(sentence))\n",
    "    d['length'] = len(word_tokenize(sentence))\n",
    "    \n",
    "    # preprocessing text to make readable for perspective api scores:\n",
    "    text = ''\n",
    "    for a in sentence:\n",
    "        if a==' ' or (a<='Z' and a>='A') or (a<='z' and a>='a') or (a<='9' and a>='0') or a=='?' or a=='.':\n",
    "            text +=a\n",
    "\n",
    "    # perspective api scores call:\n",
    "    data = '{comment: {text:\"'+text+'\"}, languages: [\"en\"], requestedAttributes: {TOXICITY:{}} }'\n",
    "    response = requests.post(url=url, data=data)\n",
    "    j = json.loads(response.content)\n",
    "    # attempting to deal with API issues\n",
    "    while 'error' in j:\n",
    "        time.sleep(5)\n",
    "        response = requests.post(url=url, data=data)\n",
    "        j = json.loads(response.content)\n",
    "    try:\n",
    "        d['toxicity'] = float(j['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "    except:\n",
    "        d['toxicity'] = 0.0\n",
    "    assert(len(d.values()) == 3)\n",
    "    return d\n",
    "\n",
    "fileobjs[0].seek(0)\n",
    "# Creating feature dict for each sample in dataset\n",
    "next(readers[0], None)\n",
    "all_data = list(readers[0])\n",
    "feature_data = {}\n",
    "for row in all_data:\n",
    "    feature_data[row[0]] = features(row[1])\n",
    "fileobjs[0].seek(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_process(num_features):\n",
    "    # Creating matrix of (samples, features) for sklearn models\n",
    "    feature_matrix = []\n",
    "    for i in range(1,1001):\n",
    "        feature_matrix.append(list(feature_data[str(i)].values()))\n",
    "    feature_matrix = numpy.array([numpy.array(x) for x in feature_matrix])\n",
    "    for i in feature_matrix:\n",
    "        if len(i) != num_features:\n",
    "            print(i) # debugging in case perspective api fails\n",
    "    return numpy.stack(feature_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.55\n",
      "non-native speaker:\n",
      "0.57\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.41\n",
      "non-native speaker:\n",
      "0.36\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.71\n",
      "non-native speaker:\n",
      "0.76\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.56\n",
      "non-native speaker:\n",
      "0.55\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.24\n",
      "non-native speaker:\n",
      "0.21\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "feature_matrix = data_process(3)\n",
    "\n",
    "L_classifiers_NS = []\n",
    "L_classifiers_NNS = []\n",
    "L_tests_NS = []\n",
    "L_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'readability':feature_matrix[:,0],\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['readability', 'length', 'toxicity']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    L_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    L_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    L_classifiers_NS.append(clfNS)\n",
    "    L_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(L_classifiers_NS[i].score(L_tests_NS[i][0], L_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(L_classifiers_NNS[i].score(L_tests_NNS[i][0], L_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Observations\n",
    "\n",
    "A naive hypothesis would assume higher accuracy for less expressive labeling schemes, but this does not always seem to be the case.\n",
    "\n",
    "In terms of accuracy, we have our Weak Neutral with the highest and Strong Neutral at the lowest. What is interesting is that the Binary and Intermediate Labeling schemes have very similar accuracies, despite being farthest apart in terms of expressiveness.\n",
    "\n",
    "### A big deciding factor of which labeling schema has the highest accuracy, appears to be how 'neutral' is expressed.\n",
    "\n",
    "EDIT: after adding partitions-based labeling, it seems to have the lowest accuracy, decreasing as we move from the Naive Bayes Classifier to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Additional Features\n",
    "\n",
    "### Adding in politeness score (from work by Prof. Danescu-Niculescu-Mizil)\n",
    "\n",
    "We are importing code from another repo focused on measuring politeness on emails.\n",
    "\n",
    "Possible issues with this approach:\n",
    "\n",
    "- Does not give a singular value measuring both politeness and impoliteness. Splitting up the scoring of a text into a separate politeness and impoliteness score might skew model results.\n",
    "\n",
    "- Words labeled as \"negative\" or \"profane\" can often be too generally applied, as they might be contained in the text but not in an offensive context. For example, the word \"black\" can be offensive in a racial context, but is often used just as a color for inanimate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.69\n",
      "non-native speaker:\n",
      "0.7\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.56\n",
      "non-native speaker:\n",
      "0.55\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.72\n",
      "non-native speaker:\n",
      "0.73\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.57\n",
      "non-native speaker:\n",
      "0.7\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.27\n",
      "non-native speaker:\n",
      "0.34\n"
     ]
    }
   ],
   "source": [
    "from Politeness_Feedback.utils import *\n",
    "\n",
    "fileobjs[0].seek(0)\n",
    "# Adding impolite and polite scores into model\n",
    "next(readers[0], None)\n",
    "all_data = list(readers[0])\n",
    "for row in all_data:\n",
    "    r = score_text(row[1])\n",
    "    feature_data[row[0]]['impolite'] = r[1]\n",
    "    feature_data[row[0]]['polite'] = r[2]\n",
    "\n",
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "feature_matrix = data_process(5)\n",
    "\n",
    "P_classifiers_NS = []\n",
    "P_classifiers_NNS = []\n",
    "P_tests_NS = []\n",
    "P_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'readability':feature_matrix[:,0],\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'impolite':feature_matrix[:,3],\n",
    "        'polite':feature_matrix[:,4],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['readability', 'length', 'toxicity', 'impolite', 'polite']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    P_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    P_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    P_classifiers_NS.append(clfNS)\n",
    "    P_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(P_classifiers_NS[i].score(P_tests_NS[i][0], P_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(P_classifiers_NNS[i].score(P_tests_NNS[i][0], P_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "\n",
    "Given our current 4 features, we will be experimenting with taking them away one-at-a-time and retraining our models to see which ones are actually useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Reading Level\n",
    "\n",
    "Because the subjects were adults with high levels of English comprehension (even non-native speakers), we hypothesize that removing this feature will not remove model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.62\n",
      "non-native speaker:\n",
      "0.7\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.47\n",
      "non-native speaker:\n",
      "0.52\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.7\n",
      "non-native speaker:\n",
      "0.74\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.56\n",
      "non-native speaker:\n",
      "0.59\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.28\n",
      "non-native speaker:\n",
      "0.31\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "no_read_classifiers_NS = []\n",
    "no_read_classifiers_NNS = []\n",
    "no_read_tests_NS = []\n",
    "no_read_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'length':feature_matrix[:,1],\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'impolite':feature_matrix[:,3],\n",
    "        'polite':feature_matrix[:,4],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['length', 'toxicity', 'impolite', 'polite']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    no_read_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    no_read_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    no_read_classifiers_NS.append(clfNS)\n",
    "    no_read_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(no_read_classifiers_NS[i].score(no_read_tests_NS[i][0], no_read_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(no_read_classifiers_NNS[i].score(no_read_tests_NNS[i][0], no_read_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Document Length\n",
    "\n",
    "We assert the experiment controlling, on average, attention spans of participants (both native and non-native). Coupled with our previous assumption on high levels of English from all participants, we hypothesize document length has a negligible effect on politeness ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.71\n",
      "non-native speaker:\n",
      "0.6\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.54\n",
      "non-native speaker:\n",
      "0.53\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.69\n",
      "non-native speaker:\n",
      "0.75\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.48\n",
      "non-native speaker:\n",
      "0.52\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.3\n",
      "non-native speaker:\n",
      "0.49\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "no_len_classifiers_NS = []\n",
    "no_len_classifiers_NNS = []\n",
    "no_len_tests_NS = []\n",
    "no_len_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'impolite':feature_matrix[:,3],\n",
    "        'polite':feature_matrix[:,4],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['toxicity', 'impolite', 'polite']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    no_len_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    no_len_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    no_len_classifiers_NS.append(clfNS)\n",
    "    no_len_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(no_len_classifiers_NS[i].score(no_len_tests_NS[i][0], no_len_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(no_len_classifiers_NNS[i].score(no_len_tests_NNS[i][0], no_len_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness vs Toxicity\n",
    "\n",
    "Both API for these features should be measuring with some similarity. By comparing accuracy between models using only one or the other, how similar are the two measures?\n",
    "\n",
    "### Toxicity Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.66\n",
      "non-native speaker:\n",
      "0.51\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.42\n",
      "non-native speaker:\n",
      "0.32\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.74\n",
      "non-native speaker:\n",
      "0.68\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.52\n",
      "non-native speaker:\n",
      "0.61\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.16\n",
      "non-native speaker:\n",
      "0.16\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "justT_classifiers_NS = []\n",
    "justT_classifiers_NNS = []\n",
    "justT_tests_NS = []\n",
    "justT_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'toxicity':feature_matrix[:,2],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['toxicity']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    justT_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    justT_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    justT_classifiers_NS.append(clfNS)\n",
    "    justT_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(justT_classifiers_NS[i].score(justT_tests_NS[i][0], justT_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(justT_classifiers_NNS[i].score(justT_tests_NNS[i][0], justT_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabeling.csv\n",
      "native speaker:\n",
      "0.65\n",
      "non-native speaker:\n",
      "0.64\n",
      "StrongNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.5\n",
      "non-native speaker:\n",
      "0.58\n",
      "WeakNeutralLabeling.csv\n",
      "native speaker:\n",
      "0.65\n",
      "non-native speaker:\n",
      "0.7\n",
      "IntermediateLabeling.csv\n",
      "native speaker:\n",
      "0.51\n",
      "non-native speaker:\n",
      "0.54\n",
      "PartitionsLabeling.csv\n",
      "native speaker:\n",
      "0.25\n",
      "non-native speaker:\n",
      "0.29\n"
     ]
    }
   ],
   "source": [
    "for i in fileobjs:\n",
    "    i.seek(0)\n",
    "\n",
    "justP_classifiers_NS = []\n",
    "justP_classifiers_NNS = []\n",
    "justP_tests_NS = []\n",
    "justP_tests_NNS = []\n",
    "for i in readers:\n",
    "    next(i, None)\n",
    "    list_data = list(i)\n",
    "    labels_NS = [row[2] for row in list_data]\n",
    "    labels_NNS = [row[3] for row in list_data]\n",
    "\n",
    "    data_NS=pd.DataFrame({\n",
    "        'impolite':feature_matrix[:,3],\n",
    "        'polite':feature_matrix[:,4],\n",
    "        'politeness': numpy.array(labels_NS)\n",
    "    })\n",
    "    data_NS.head()\n",
    "    data_NNS=pd.DataFrame({\n",
    "        'politeness': numpy.array(labels_NNS)\n",
    "    })\n",
    "    data_NNS.head()\n",
    "    X=data_NS[['impolite', 'polite']]\n",
    "\n",
    "    # NS training\n",
    "    # Splitting up into 85% training, 15% verification\n",
    "    NS_xtrain, NS_xtest, NS_ytrain, NS_ytest = train_test_split(X, data_NS['politeness'], test_size=0.1)\n",
    "    justP_tests_NS.append((NS_xtest, NS_ytest))\n",
    "    \n",
    "    # NNS training\n",
    "    NNS_xtrain, NNS_xtest, NNS_ytrain, NNS_ytest = train_test_split(X, data_NNS['politeness'], test_size=0.1)\n",
    "    justP_tests_NNS.append((NNS_xtest, NNS_ytest))\n",
    "\n",
    "    clfNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNS.fit(NS_xtrain, NS_ytrain)\n",
    "    clfNNS = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clfNNS.fit(NNS_xtrain, NNS_ytrain)\n",
    "    justP_classifiers_NS.append(clfNS)\n",
    "    justP_classifiers_NNS.append(clfNNS)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    print(\"native speaker:\")\n",
    "    print(justP_classifiers_NS[i].score(justP_tests_NS[i][0], justP_tests_NS[i][1]))\n",
    "    print(\"non-native speaker:\")\n",
    "    print(justP_classifiers_NNS[i].score(justP_tests_NNS[i][0], justP_tests_NNS[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
